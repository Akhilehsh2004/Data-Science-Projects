{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a8ddb5-6cc3-406e-a0a5-1b80171f9311",
   "metadata": {},
   "source": [
    "## Akhilesh Pant (AU FTCA: MCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8406e91-5e94-45ec-8c81-b8e39afc52b0",
   "metadata": {},
   "source": [
    "# AI-Powered Threat (Weapons) Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e44cd-77e6-434b-9ae6-4ad8873120cf",
   "metadata": {},
   "source": [
    "## Using Live Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b27ea6-92d2-43df-a996-8dea5fb6c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 images belonging to 3 classes.\n",
      "Found 420 images belonging to 3 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │           \u001b[38;5;34m5,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m163,968\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m387\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,219,046</span> (16.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,219,046\u001b[0m (16.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,915</span> (652.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m166,915\u001b[0m (652.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,052,131</span> (15.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,052,131\u001b[0m (15.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jagda\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# ✅ Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "# ✅ Dataset Loading and Preprocessing\n",
    "data_dir = 'dataset12'\n",
    "img_size = (224, 224)\n",
    "batch_size = 64\n",
    "\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=40,\n",
    "    zoom_range=0.4,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = data_gen.flow_from_directory(\n",
    "    data_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='training')\n",
    "val_generator = data_gen.flow_from_directory(\n",
    "    data_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='validation')\n",
    "\n",
    "# ✅ Build the Transfer Learning Model with EfficientNetB0\n",
    "num_classes = train_generator.num_classes\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ✅ Callbacks for Better Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6),\n",
    "    ModelCheckpoint('best_threat_detection_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# ✅ Train the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# ✅ Visualize Training Performance\n",
    "def plot_performance(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(history)\n",
    "\n",
    "# ✅ Save the Final Model\n",
    "model.save('final_threat_detection_model.h5')\n",
    "print(\"Final model saved successfully!\")\n",
    "\n",
    "# ✅ Live Camera Threat Detection\n",
    "def detect_live_threat():\n",
    "    loaded_model = tf.keras.models.load_model('final_threat_detection_model.h5')\n",
    "    class_labels = list(train_generator.class_indices.keys())\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = cv2.resize(frame, img_size)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        prediction = loaded_model.predict(img, verbose=0)[0]\n",
    "        label_idx = np.argmax(prediction)\n",
    "        label = class_labels[label_idx]\n",
    "        confidence = prediction[label_idx] * 100\n",
    "\n",
    "        color = (0, 0, 255) if label == \"armed_threat\" else ((0, 255, 0) if label == \"unarmed_individual\" else (255, 255, 0))\n",
    "        cv2.putText(frame, f'{label}: {confidence:.2f}%', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        cv2.imshow('Threat Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start live detection\n",
    "detect_live_threat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87372a47-c11e-4f0e-a434-97e8ddd14fe1",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418ba22-defa-4289-b6f3-1a290a9e814a",
   "metadata": {},
   "source": [
    "Here's an in-depth, line-by-line explanation of your threat detection code, covering key concepts and how each section contributes to the overall functionality.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **1. Import Required Libraries**  \n",
    "These libraries are essential for data processing, model building, visualization, and live camera detection.\n",
    "\n",
    "```python\n",
    "import numpy as np                # For numerical operations\n",
    "import pandas as pd               # For data manipulation (not directly used here)\n",
    "import matplotlib.pyplot as plt   # For plotting training graphs\n",
    "import tensorflow as tf           # For deep learning model creation and training\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For image augmentation\n",
    "from tensorflow.keras.applications import EfficientNetB0              # Pre-trained model for transfer learning\n",
    "from tensorflow.keras.models import Sequential                       # Sequential model API\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization  # Neural network layers\n",
    "from tensorflow.keras.optimizers import Adam                          # Optimizer for model training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # Callbacks for better training control\n",
    "from sklearn.metrics import classification_report, confusion_matrix   # For evaluation metrics (though not used here)\n",
    "import seaborn as sns              # For advanced visualizations (not directly used here)\n",
    "import cv2                         # For capturing video from the webcam\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **2. Dataset Loading and Preprocessing**  \n",
    "This section handles data preparation and augmentation.\n",
    "\n",
    "```python\n",
    "data_dir = 'dataset12'            # Path to the image dataset\n",
    "img_size = (224, 224)             # Target image size for the model\n",
    "batch_size = 64                   # Number of images processed at once during training\n",
    "```\n",
    "\n",
    "**Image Augmentation for Better Generalization**  \n",
    "- Helps the model become robust by modifying images slightly during training.  \n",
    "\n",
    "```python\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,               # Normalizes pixel values to the range [0, 1]\n",
    "    validation_split=0.2,         # Splits 20% of the data for validation\n",
    "    rotation_range=40,            # Randomly rotates images by 40 degrees\n",
    "    zoom_range=0.4,               # Randomly zooms images by 40%\n",
    "    width_shift_range=0.3,        # Random horizontal shift\n",
    "    height_shift_range=0.3,       # Random vertical shift\n",
    "    shear_range=0.3,              # Shear transformations for distortion\n",
    "    horizontal_flip=True,         # Randomly flip images horizontally\n",
    "    fill_mode='nearest'           # Filling strategy for new pixels after transformation\n",
    ")\n",
    "```\n",
    "\n",
    "**Data Generators**  \n",
    "- Splits data into training and validation sets.\n",
    "\n",
    "```python\n",
    "train_generator = data_gen.flow_from_directory(\n",
    "    data_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='training')\n",
    "val_generator = data_gen.flow_from_directory(\n",
    "    data_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', subset='validation')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **3. Build the Transfer Learning Model with EfficientNetB0**  \n",
    "Using a pre-trained model for faster and efficient learning.\n",
    "\n",
    "```python\n",
    "num_classes = train_generator.num_classes  # Automatically detects the number of classes\n",
    "```\n",
    "\n",
    "**Loading EfficientNetB0 Model**  \n",
    "- `include_top=False` excludes the default classification layers.  \n",
    "- `trainable=False` freezes the base layers to retain pre-trained knowledge.  \n",
    "\n",
    "```python\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "```\n",
    "\n",
    "**Custom Top Layers**  \n",
    "- These layers are added for classification specific to the custom dataset.  \n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),        # Reduces the feature map to a single vector\n",
    "    BatchNormalization(),            # Normalizes activations and accelerates training\n",
    "    Dropout(0.5),                    # Prevents overfitting by randomly dropping neurons\n",
    "    Dense(128, activation='relu'),   # Fully connected layer with ReLU activation\n",
    "    Dropout(0.3),                    # Additional dropout for regularization\n",
    "    Dense(num_classes, activation='softmax')  # Final layer for multi-class classification\n",
    "])\n",
    "```\n",
    "\n",
    "**Model Compilation**  \n",
    "- `Adam` optimizer with a learning rate of 0.0001.  \n",
    "- `categorical_crossentropy` is used for multi-class classification.  \n",
    "\n",
    "```python\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **4. Callbacks for Better Training**  \n",
    "Callbacks help optimize training and prevent overfitting.\n",
    "\n",
    "```python\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),  # Stops early if validation loss doesn't improve\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6),  # Reduces learning rate when a metric stops improving\n",
    "    ModelCheckpoint('best_threat_detection_model.h5', monitor='val_loss', save_best_only=True)  # Saves the best model\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **5. Train the Model**  \n",
    "Begins the training process.\n",
    "\n",
    "```python\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **6. Visualize Training Performance**  \n",
    "Plots accuracy and loss for both training and validation data.\n",
    "\n",
    "```python\n",
    "def plot_performance(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(history)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **7. Save the Final Model**  \n",
    "Saves the trained model for later use.\n",
    "\n",
    "```python\n",
    "model.save('final_threat_detection_model.h5')\n",
    "print(\"Final model saved successfully!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **8. Live Camera Threat Detection**  \n",
    "Loads the model and performs real-time threat detection.\n",
    "\n",
    "```python\n",
    "def detect_live_threat():\n",
    "    loaded_model = tf.keras.models.load_model('final_threat_detection_model.h5')\n",
    "    class_labels = list(train_generator.class_indices.keys())  # Maps class indices to labels\n",
    "    cap = cv2.VideoCapture(0)  # Opens the default camera\n",
    "```\n",
    "\n",
    "**Continuous Frame Processing**  \n",
    "\n",
    "```python\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Reads a frame\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = cv2.resize(frame, img_size)   # Resizes the frame to match model input\n",
    "        img = img.astype('float32') / 255.0  # Normalizes pixel values\n",
    "        img = np.expand_dims(img, axis=0)    # Adds batch dimension\n",
    "```\n",
    "\n",
    "**Model Prediction and Display**  \n",
    "\n",
    "```python\n",
    "        prediction = loaded_model.predict(img, verbose=0)[0]  # Predicts the class\n",
    "        label_idx = np.argmax(prediction)                     # Gets the index of the highest probability\n",
    "        label = class_labels[label_idx]                        # Maps index to label\n",
    "        confidence = prediction[label_idx] * 100               # Converts confidence to percentage\n",
    "\n",
    "        color = (0, 0, 255) if label == \"armed_threat\" else ((0, 255, 0) if label == \"unarmed_individual\" else (255, 255, 0))\n",
    "        cv2.putText(frame, f'{label}: {confidence:.2f}%', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)  # Displays result on frame\n",
    "        cv2.imshow('AI-Powered Threat (Weapons) Detection', frame)  # Shows the video frame\n",
    "```\n",
    "\n",
    "**Exit Mechanism**  \n",
    "\n",
    "```python\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()              # Releases the camera resource\n",
    "    cv2.destroyAllWindows()    # Closes all OpenCV windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **9. Start Live Detection**  \n",
    "\n",
    "```python\n",
    "detect_live_threat()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This code effectively implements a **real-time AI-powered threat detection system** using a transfer learning approach with `EfficientNetB0`. The process involves data preprocessing, model training with augmentation, performance visualization, and live detection using a webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e42c92-47ee-4dd7-bddc-31fcb1317224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────── Imports ─────────────────────────\n",
    "import os, random, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, BatchNormalization,\n",
    "    Dropout, Dense\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    ")\n",
    "\n",
    "# ────────────────────── Reproducibility ────────────────────\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# ───────────────────── Data Load & Prep ────────────────────\n",
    "data_dir   = \"dataset12\"       # <— update if necessary\n",
    "img_size   = (224, 224)\n",
    "batch_size = 64\n",
    "val_split  = 0.20\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale            = 1./255,\n",
    "    validation_split   = val_split,\n",
    "    rotation_range     = 40,\n",
    "    zoom_range         = 0.4,\n",
    "    width_shift_range  = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    shear_range        = 0.3,\n",
    "    horizontal_flip    = True,\n",
    "    fill_mode          = \"nearest\"\n",
    ")\n",
    "\n",
    "# **No augmentation for validation — only rescale**\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale          = 1./255,\n",
    "    validation_split = val_split\n",
    ")\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size = img_size,\n",
    "    batch_size  = batch_size,\n",
    "    class_mode  = \"categorical\",\n",
    "    subset      = \"training\",\n",
    "    shuffle     = True,\n",
    "    seed        = seed\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size = img_size,\n",
    "    batch_size  = batch_size,\n",
    "    class_mode  = \"categorical\",\n",
    "    subset      = \"validation\",\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "num_classes      = train_gen.num_classes\n",
    "steps_per_epoch  = train_gen.samples // batch_size\n",
    "validation_steps = val_gen.samples  // batch_size\n",
    "\n",
    "# ──────────────────────── Model ────────────────────────────\n",
    "base_model = EfficientNetB0(\n",
    "    weights     = \"imagenet\",\n",
    "    include_top = False,\n",
    "    input_shape = img_size + (3,)\n",
    ")\n",
    "base_model.trainable = False     # freeze backbone\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.50),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.30),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=1e-4),\n",
    "    loss      = \"categorical_crossentropy\",\n",
    "    metrics   = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ────────────────────── Callbacks ──────────────────────────\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor           = \"val_loss\",\n",
    "        patience          = 5,\n",
    "        restore_best_weights = True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor   = \"val_loss\",\n",
    "        factor    = 0.2,\n",
    "        patience  = 3,\n",
    "        min_lr    = 1e-6\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        \"best_threat_detection_model.h5\",\n",
    "        monitor        = \"val_loss\",\n",
    "        save_best_only = True\n",
    "    )\n",
    "]\n",
    "\n",
    "# ─────────────────────── Training ──────────────────────────\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs            = 25,\n",
    "    steps_per_epoch   = steps_per_epoch,\n",
    "    validation_data   = val_gen,\n",
    "    validation_steps  = validation_steps,\n",
    "    callbacks         = callbacks,\n",
    "    verbose           = 1\n",
    ")\n",
    "\n",
    "# ──────────────────── Performance Plot ─────────────────────\n",
    "def plot_performance(hist):\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(hist.history[\"accuracy\"],     label=\"Train\")\n",
    "    plt.plot(hist.history[\"val_accuracy\"], label=\"Val\")\n",
    "    plt.title(\"Accuracy\");  plt.xlabel(\"Epoch\");  plt.ylabel(\"Acc\");  plt.legend()\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(hist.history[\"loss\"],     label=\"Train\")\n",
    "    plt.plot(hist.history[\"val_loss\"], label=\"Val\")\n",
    "    plt.title(\"Loss\");  plt.xlabel(\"Epoch\");  plt.ylabel(\"Loss\");  plt.legend()\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_performance(history)\n",
    "\n",
    "# ─────────────────── Save Final Model ──────────────────────\n",
    "model.save(\"final_threat_detection_model.h5\")\n",
    "print(\"✅ Model saved to final_threat_detection_model.h5\")\n",
    "\n",
    "# ─────────────── Live‑Camera Threat Detection ──────────────\n",
    "def detect_live_threat():\n",
    "    loaded_model = tf.keras.models.load_model(\"final_threat_detection_model.h5\")\n",
    "    class_labels = list(train_gen.class_indices.keys())\n",
    "\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # CAP_DSHOW=✓ on Windows; harmless elsewhere\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"❌ Cannot open webcam\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "\n",
    "            img = cv2.resize(frame, img_size)\n",
    "            img = img.astype(\"float32\") / 255.0\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "\n",
    "            preds = loaded_model.predict(img, verbose=0)[0]\n",
    "            idx   = int(np.argmax(preds))\n",
    "            label = class_labels[idx]\n",
    "            conf  = preds[idx] * 100\n",
    "\n",
    "            # Simple colour mapping (extend as needed)\n",
    "            colours = {\n",
    "                \"armed_threat\":      (  0,   0, 255),  # red\n",
    "                \"unarmed_individual\":(  0, 255,   0)   # green\n",
    "            }\n",
    "            colour = colours.get(label, (255,255,0))\n",
    "\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"{label}: {conf:.2f}%\",\n",
    "                (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                colour,\n",
    "                2\n",
    "            )\n",
    "            cv2.imshow(\"Live Threat Detection\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"): break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# ─────────────────────── Run it ────────────────────────────\n",
    "detect_live_threat()   # <- uncomment when you're ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa0f71-1bc2-4b10-8bb1-3b4d14ce312e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
